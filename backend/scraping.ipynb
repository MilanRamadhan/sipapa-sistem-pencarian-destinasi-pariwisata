{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21df367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ============================\n",
    "# PATH FILE\n",
    "# ============================\n",
    "BASE_DIR = Path.cwd()\n",
    "URL_FILE = BASE_DIR / \"data\" / \"urls.txt\"\n",
    "OUTPUT_FILE = BASE_DIR / \"data\" / \"scraped.csv\"\n",
    "\n",
    "OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# PARAMETER SCRAPING\n",
    "# ============================\n",
    "REQUEST_TIMEOUT = 15  # detik\n",
    "SLEEP_BETWEEN_REQUESTS = 0.2  # jeda antar request biar sopan\n",
    "MIN_WORDS = 40  # minimal jumlah kata supaya artikel dianggap valid\n",
    "\n",
    "# User-Agent biar nggak keliatan terlalu \"robot\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# BACA URL DARI urls.txt\n",
    "# ============================\n",
    "def load_urls(path: Path) -> list[str]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File URL tidak ditemukan: {path}\")\n",
    "    \n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        raw = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Hapus duplikat tapi jaga urutan\n",
    "    seen = set()\n",
    "    urls = []\n",
    "    for u in raw:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            urls.append(u)\n",
    "    \n",
    "    print(f\"[INFO] Total URL di {path.name}: {len(urls)}\")\n",
    "    return urls\n",
    "\n",
    "# ============================\n",
    "# FILTER: CUMA AMBIL URL ARTIKEL\n",
    "# ============================\n",
    "def is_article_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Filter URL supaya:\n",
    "    - Nggak ambil /copy/, /komentar/, /image/, /search/ (bukan artikel utama)\n",
    "    - Nggak ambil homepage / halaman root\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path or \"\"\n",
    "\n",
    "    # Buang path yang jelas bukan artikel\n",
    "    blocked_fragments = [\n",
    "        \"/copy/\",\n",
    "        \"/komentar/\",\n",
    "        \"/image/\",\n",
    "        \"/search/\",\n",
    "    ]\n",
    "    if any(b in path for b in blocked_fragments):\n",
    "        return False\n",
    "\n",
    "    # Buang homepage dan path kosong\n",
    "    if path in [\"\", \"/\"]:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# ============================\n",
    "# CLEAN KONTEN ARTIKEL\n",
    "# ============================\n",
    "def clean_content(content: str, title: str) -> str:\n",
    "    \"\"\"\n",
    "    Bersihkan konten artikel dari noise:\n",
    "    - Judul duplikat di awal konten\n",
    "    - 'Baca juga:' dan link-nya\n",
    "    - Caption Instagram 'Sebuah kiriman dibagikan oleh'\n",
    "    - Copyright footer\n",
    "    - Editor/Tim Redaksi di awal\n",
    "    - Whitespace berlebih\n",
    "    \"\"\"\n",
    "    if not content:\n",
    "        return \"\"\n",
    "    \n",
    "    # Hapus judul duplikat di awal (case-insensitive)\n",
    "    if title:\n",
    "        # Hapus judul yang muncul di awal konten\n",
    "        lines = content.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        skip_count = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line_lower = line.strip().lower()\n",
    "            title_lower = title.strip().lower()\n",
    "            \n",
    "            # Skip jika line adalah judul atau judul duplikat\n",
    "            if skip_count < 3 and (line_lower == title_lower or title_lower in line_lower and len(line_lower) < len(title_lower) + 20):\n",
    "                skip_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Skip editor/tim redaksi di awal\n",
    "            if skip_count < 2 and (line_lower in ['editor', 'tim redaksi', 'reporter']):\n",
    "                skip_count += 1\n",
    "                continue\n",
    "                \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        content = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # Hapus 'Baca juga:' beserta linknya (biasanya satu baris)\n",
    "    content = re.sub(r'Baca juga:.*?(?=\\n|$)', '', content, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Hapus caption Instagram\n",
    "    content = re.sub(r'Sebuah kiriman dibagikan oleh.*?(?=\\n|$)', '', content, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Hapus copyright footer\n",
    "    content = re.sub(r'Copyright \\d{4}.*?All Rights Reserved\\.?', '', content, flags=re.IGNORECASE)\n",
    "    content = re.sub(r'©.*?\\d{4}.*?(?=\\n|$)', '', content, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Hapus caption gambar yang sering mengganggu (KOMPAS.COM/NAMA)\n",
    "    content = re.sub(r'[A-Z]+\\.[A-Z]+/[A-Z]+(?=[A-Z])', '', content)\n",
    "    \n",
    "    # Rapikan whitespace\n",
    "    content = re.sub(r'\\n\\s*\\n', '\\n\\n', content)  # max 2 newlines\n",
    "    content = re.sub(r' +', ' ', content)  # single space\n",
    "    content = content.strip()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# ============================\n",
    "# EXTRACT GAMBAR ARTIKEL\n",
    "# ============================\n",
    "def extract_images(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Ekstrak URL gambar utama artikel:\n",
    "    1. Dari meta og:image (Open Graph)\n",
    "    2. Dari img tag pertama di article/content area\n",
    "    \"\"\"\n",
    "    # Prioritas 1: Open Graph image\n",
    "    og_image = soup.find(\"meta\", attrs={\"property\": \"og:image\"})\n",
    "    if og_image and og_image.get(\"content\"):\n",
    "        return og_image[\"content\"].strip()\n",
    "    \n",
    "    # Prioritas 2: Twitter card image\n",
    "    twitter_image = soup.find(\"meta\", attrs={\"name\": \"twitter:image\"})\n",
    "    if twitter_image and twitter_image.get(\"content\"):\n",
    "        return twitter_image[\"content\"].strip()\n",
    "    \n",
    "    # Prioritas 3: Gambar pertama di area artikel\n",
    "    # Coba cari di container artikel dulu\n",
    "    article_containers = soup.find_all([\"article\", \"div\"], class_=lambda x: x and ('article' in x.lower() or 'content' in x.lower()))\n",
    "    for container in article_containers:\n",
    "        img = container.find(\"img\", src=True)\n",
    "        if img and img.get(\"src\"):\n",
    "            src = img[\"src\"].strip()\n",
    "            # Skip gambar kecil/icon (biasanya <100px atau base64)\n",
    "            if not src.startswith('data:') and 'icon' not in src.lower() and 'logo' not in src.lower():\n",
    "                return src\n",
    "    \n",
    "    # Fallback: gambar pertama di seluruh halaman\n",
    "    img = soup.find(\"img\", src=True)\n",
    "    if img and img.get(\"src\"):\n",
    "        src = img[\"src\"].strip()\n",
    "        if not src.startswith('data:'):\n",
    "            return src\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# ============================\n",
    "# EXTRACT TEKS DARI HTML\n",
    "# ============================\n",
    "def extract_article(url: str, html: str) -> dict:\n",
    "    \"\"\"\n",
    "    Ambil title + content (gabungan <p>) + image dari halaman.\n",
    "    Untuk detikTravel & KompasTravel, ambil semua <p> sudah cukup bagus.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Title\n",
    "    title = None\n",
    "\n",
    "    # Coba dari <meta property=\"og:title\">\n",
    "    og_title = soup.find(\"meta\", attrs={\"property\": \"og:title\"})\n",
    "    if og_title and og_title.get(\"content\"):\n",
    "        title = og_title[\"content\"].strip()\n",
    "\n",
    "    # Kalau belum ketemu, pakai <title>\n",
    "    if not title:\n",
    "        title_tag = soup.find(\"title\")\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "\n",
    "    if not title:\n",
    "        title = \"No Title\"\n",
    "\n",
    "    # Kumpulin semua <p>\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
    "    paragraphs = [p for p in paragraphs if p]\n",
    "    content = \"\\n\".join(paragraphs)\n",
    "    \n",
    "    # Bersihkan konten dari noise\n",
    "    content = clean_content(content, title)\n",
    "    \n",
    "    # Ekstrak gambar artikel\n",
    "    image_url = extract_images(soup)\n",
    "\n",
    "    # Hitung jumlah kata, buat ngecek minimal\n",
    "    word_count = len(content.split())\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"domain\": urlparse(url).netloc,\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"image_url\": image_url,\n",
    "        \"word_count\": word_count,\n",
    "        \"timestamp\": time.time(),\n",
    "    }\n",
    "\n",
    "# ============================\n",
    "# HTTP REQUEST\n",
    "# ============================\n",
    "def fetch(url: str) -> str | None:\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "        if res.status_code == 200:\n",
    "            return res.text\n",
    "        else:\n",
    "            print(f\"[STATUS {res.status_code}] {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================\n",
    "# SCRAPE SATU URL\n",
    "# ============================\n",
    "def scrape_one(url: str) -> dict | None:\n",
    "    print(f\"[SCRAPE] {url}\")\n",
    "    html = fetch(url)\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    article = extract_article(url, html)\n",
    "\n",
    "    # Skip jika konten terlalu pendek\n",
    "    if not article[\"content\"] or article[\"word_count\"] < MIN_WORDS:\n",
    "        print(f\"[SKIP - KONTEN PENDEK] {url}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"[OK] {url} (≈ {article['word_count']} kata)\")\n",
    "    return article\n",
    "\n",
    "# ============================\n",
    "# MAIN: LOOP SCRAPING\n",
    "# ============================\n",
    "def scrape_all():\n",
    "    urls = load_urls(URL_FILE)\n",
    "\n",
    "    # Filter URL supaya cuma artikel utama\n",
    "    article_urls = [u for u in urls if is_article_url(u)]\n",
    "    print(f\"[INFO] URL yang lolos filter pola artikel: {len(article_urls)}\")\n",
    "\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, url in enumerate(article_urls, start=1):\n",
    "        print(f\"\\n[{idx}/{len(article_urls)}]\")\n",
    "        data = scrape_one(url)\n",
    "        if data:\n",
    "            results.append(data)\n",
    "\n",
    "        # Jeda kecil antar request\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n[INFO] Scraping selesai dalam {elapsed:.2f} detik.\")\n",
    "    print(f\"[INFO] Total artikel valid: {len(results)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================\n",
    "# EKSEKUSI UTAMA (BISA DI-RUN DI JUPYTER)\n",
    "# ============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[INFO] Working Directory : {BASE_DIR}\")\n",
    "    print(f\"[INFO] URL source       : {URL_FILE}\")\n",
    "    print(f\"[INFO] Output CSV       : {OUTPUT_FILE}\\n\")\n",
    "\n",
    "    results = scrape_all()\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\n[SUCCESS] Scraping selesai! File disimpan ke: {OUTPUT_FILE}\")\n",
    "        print(f\"[INFO] Total data berhasil di-scrape: {len(results)}\")\n",
    "\n",
    "        # Preview\n",
    "        print(\"\\n[PREVIEW] 5 data pertama:\")\n",
    "        display(df.head())\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Tidak ada data yang berhasil di-scrape!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
